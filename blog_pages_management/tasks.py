from celery import shared_task
from django.utils.timezone import now, timedelta
from django.core.mail import send_mail
from django.contrib.auth import get_user_model
from .models import BlogPost, AdminNotification, Newsletter, NewsletterSubscriber
from django.db.models import F
from PIL import Image
from io import BytesIO
from django.core.files.base import ContentFile
from django.core.cache import cache
import requests
from bs4 import BeautifulSoup
from datetime import datetime
from itertools import islice
import spacy
import numpy as np
import pandas as pd
from surprise import SVD, Dataset, Reader
from celery import shared_task
from django.core.cache import cache
from .models import BlogClick

nlp = spacy.load("en_core_web_md")
User = get_user_model()

@shared_task
def generate_blog_embeddings():
    """Computes and stores NLP-based vector embeddings for all blogs."""
    blogs = BlogPost.objects.filter(embedding__isnull=True)
    for blog in blogs:
        doc = nlp(blog.content)
        blog.embedding = doc.vector.tolist()  # Store vector representation
        blog.save(update_fields=["embedding"])

    return f"Generated embeddings for {blogs.count()} blogs."

@shared_task
def find_similar_blogs(blog_id):
    """Finds and caches similar blogs using content similarity."""
    blog = BlogPost.objects.get(id=blog_id)
    if not blog.embedding:
        return "No embedding found for this blog."

    all_blogs = BlogPost.objects.exclude(id=blog.id).exclude(embedding__isnull=True)
    
    similarities = [
        (
            other_blog,
            np.dot(blog.embedding, other_blog.embedding) / (np.linalg.norm(blog.embedding) * np.linalg.norm(other_blog.embedding))
        )
        for other_blog in all_blogs
    ]
    
    sorted_blogs = sorted(similarities, key=lambda x: x[1], reverse=True)[:5]
    similar_blog_data = [{"title": b[0].title, "slug": b[0].slug, "similarity": b[1]} for b in sorted_blogs]

    cache.set(f"similar_blogs_{blog.id}", similar_blog_data, timeout=3600)
    return f"Cached similar blogs for {blog.title}."

@shared_task
def train_blog_recommendation_model():
    """Trains a collaborative filtering model for personalized blog recommendations."""
    data = BlogClick.objects.values("user_id", "blog_id")
    df = pd.DataFrame(data)

    reader = Reader(rating_scale=(1, 1))  # Binary rating (clicked/not clicked)
    dataset = Dataset.load_from_df(df[["user_id", "blog_id"]], reader)

    trainset = dataset.build_full_trainset()
    model = SVD()
    model.fit(trainset)

    cache.set("blog_recommendation_model", model, timeout=86400)
    return "Trained blog recommendation model."

@shared_task
def recommend_blogs_for_user(user_id):
    """Generates personalized blog recommendations using an ML model."""
    model = cache.get("blog_recommendation_model")
    if not model:
        return "No trained model found."

    blogs = BlogPost.objects.all()
    predictions = [(blog, model.predict(user_id, blog.id).est) for blog in blogs]
    sorted_blogs = sorted(predictions, key=lambda x: x[1], reverse=True)[:5]

    recommended_blog_data = [{"title": b[0].title, "slug": b[0].slug, "score": b[1]} for b in sorted_blogs]
    cache.set(f"user_recommendations_{user_id}", recommended_blog_data, timeout=86400)
    
    return f"Generated recommendations for User {user_id}."


@shared_task
def auto_delete_soft_deleted_blogs():
    """Deletes blogs that have been soft-deleted for over 30 days."""
    expiration_date = now() - timedelta(days=30)
    deleted_blogs = BlogPost.objects.filter(is_deleted=True, deleted_at__lte=expiration_date)

    if not deleted_blogs.exists():
        return "No soft-deleted blogs to remove."

    count = deleted_blogs.count()
    blog_titles = list(deleted_blogs.values_list("title", flat=True))

    # Bulk delete
    deleted_blogs.delete()

    # Send notifications to all admins
    admin_emails = User.objects.filter(is_staff=True, is_superuser=True, email__isnull=False).values_list("email", flat=True)

    if admin_emails:
        send_mail(
            subject="Auto-Deleted Soft-Deleted Blogs",
            message=f"The following {count} soft-deleted blogs were permanently removed:\n\n" + "\n".join(blog_titles),
            from_email="no-reply@yourdomain.com",
            recipient_list=list(admin_emails),
            fail_silently=True,
        )

    return f"Auto-deleted {count} expired soft-deleted blog posts."


@shared_task
def warn_admins_before_deletion():
    """Notifies admins 7 days before soft-deleted blogs are permanently removed."""
    warning_threshold = now() - timedelta(days=23)  # 7 days before auto-delete
    expiring_blogs = BlogPost.objects.filter(is_deleted=True, deleted_at__lte=warning_threshold)

    if not expiring_blogs.exists():
        return "No blogs require admin warnings."

    count = expiring_blogs.count()
    admins = User.objects.filter(is_staff=True, is_superuser=True)

    notifications = [
        AdminNotification(user=admin, message=f"Warning: {count} blogs will be permanently deleted in 7 days. Review and restore if necessary.")
        for admin in admins
    ]

    AdminNotification.objects.bulk_create(notifications)  # Bulk insert for efficiency
    return f"Notified admins about {count} soon-to-be-deleted blogs."


@shared_task
def update_freshness_scores():
    """
    Updates the freshness scores of all blogs daily.
    """
    blogs = BlogPost.objects.all()
    updated_count = 0

    for blog in blogs:
        days_since_update = (now() - blog.updated_at).days if blog.updated_at else 365
        engagement_decay = max(0, 10 - blog.click_count)

        new_freshness_score = max(0, 100 - (days_since_update * 2) - engagement_decay)

        if new_freshness_score < 40:
            AdminNotification.objects.create(
                user=blog.last_edited_by,
                message=f"Blog '{blog.title}' needs an update. Freshness Score: {new_freshness_score}"
            )

        blog.freshness_score = new_freshness_score
        blog.save(update_fields=["freshness_score"])
        updated_count += 1

    return f"Updated freshness scores for {updated_count} blogs."


@shared_task
def reset_click_counters():
    """Resets daily, weekly, and monthly click counts at correct intervals."""
    today = datetime.today().weekday()  # 0 = Monday, 6 = Sunday

    # Reset daily clicks
    BlogPost.objects.update(daily_clicks=0)

    # Reset weekly clicks only on **Monday**
    if today == 0:
        BlogPost.objects.update(weekly_clicks=0)

    # Reset monthly clicks only on the **1st of the month**
    if now().day == 1:
        BlogPost.objects.update(monthly_clicks=0)

    return "Click counters reset successfully."



@shared_task
def compress_image_task(image_field):
    """Compresses and resizes an image asynchronously before saving."""
    img = Image.open(image_field)

    img = img.convert("RGB")
    max_width = 1200
    if img.width > max_width:
        ratio = max_width / img.width
        img = img.resize((max_width, int(img.height * ratio)), Image.ANTIALIAS)

    buffer = BytesIO()
    img.save(buffer, format="JPEG", quality=80)
    buffer.seek(0)

    # Save back to the model field
    image_field.save(image_field.name, ContentFile(buffer.getvalue()), save=True)

    return f"Image {image_field.name} compressed successfully."


@shared_task
def send_newsletter(newsletter_id):
    """Sends newsletters in smaller batches to prevent email flooding."""
    newsletter = Newsletter.objects.get(id=newsletter_id)
    emails = list(NewsletterSubscriber.objects.filter(
        website=newsletter.website, is_active=True
    ).values_list("email", flat=True))

    batch_size = 100
    for i in range(0, len(emails), batch_size):
        send_mail(
            subject=newsletter.subject_a,
            message=newsletter.content_a,
            from_email="newsletters@yourdomain.com",
            recipient_list=emails[i:i + batch_size],  # Send in chunks
            fail_silently=True,
        )

    return f"Newsletter sent to {len(emails)} subscribers."



# send weekly newsletter
@shared_task
def send_weekly_newsletter():
    """Sends the latest published blogs to newsletter subscribers every week."""
    one_week_ago = now() - timedelta(days=7)
    blogs = BlogPost.objects.filter(is_published=True, publish_date__gte=one_week_ago)

    emails = NewsletterSubscriber.objects.filter(is_active=True).values_list("email", flat=True)

    if blogs.exists():
        blog_list = "\n".join([f"- {blog.title}: {blog.get_absolute_url()}" for blog in blogs])
        message = f"Here are the latest blogs from this week:\n\n{blog_list}"
        
        send_mail(
            subject="ðŸ”¥ Weekly Blog Roundup",
            message=message,
            from_email="newsletters@yourdomain.com",
            recipient_list=list(emails),
            fail_silently=True,
        )


@shared_task
def update_blog_engagement_cache():
    """Batch update blog engagement stats in Redis."""
    blogs = BlogPost.objects.filter(is_published=True).values("id", "click_count", "conversion_count")
    for blog in blogs:
        cache.set(f"blog_engagement_{blog['id']}", blog, timeout=600)

@shared_task
def publish_scheduled_blogs():
    """Automatically publishes scheduled blog posts efficiently."""
    now_time = now()
    BlogPost.objects.filter(is_published=False, scheduled_publish_date__lte=now_time).update(
        is_published=True, publish_date=now_time
    )

@shared_task
def check_for_broken_links():
    """Scans blog content for broken links and alerts admins."""
    for blog in BlogPost.objects.filter(is_published=True):
        soup = BeautifulSoup(blog.content, "html.parser")
        links = [a["href"] for a in soup.find_all("a", href=True)]

        broken_links = []
        for link in links:
            try:
                response = requests.head(link, timeout=5)
                if response.status_code >= 400:
                    broken_links.append(link)
            except requests.RequestException:
                broken_links.append(link)

        if broken_links:
            for admin in User.objects.filter(is_staff=True):
                message = f"Blog '{blog.title}' contains {len(broken_links)} broken links."
                AdminNotification.objects.create(user=admin, message=message)


@shared_task
def update_engagement_stats():
    """Background task to cache trending and engaging blogs."""
    last_week = now() - timedelta(days=7)
    blogs = BlogPost.objects.filter(updated_at__gte=last_week).only("title", "slug", "click_count", "conversion_count")

    trending_blogs = blogs.order_by("-click_count")[:5]
    most_engaging_blogs = blogs.order_by("-conversion_count")[:5]


    engagement_stats = {
        "trending_blogs": [{"title": blog.title, "slug": blog.slug, "clicks": blog.click_count} for blog in trending_blogs],
        "most_engaging_blogs": [{"title": blog.title, "slug": blog.slug, "conversions": blog.conversion_count} for blog in most_engaging_blogs],
    }

    # Store new stats & version
    cache.set("engagement_stats", engagement_stats, timeout=3600)  # Cache for 1 hour
    cache.set("engagement_version", now().timestamp(), timeout=3600)  # Store new version for ETag


@shared_task
def sync_blog_clicks_to_db():
    """
    Periodically syncs blog click counts from Redis to the database in bulk.
    """
    updated_blogs = []
    cache_keys = cache.keys("blog_clicks_*")  # Fetch all cached click keys

    for cache_key in cache_keys:
        blog_id = cache_key.split("_")[-1]  # Extract blog ID from key
        click_count = cache.get(cache_key)

        if click_count:
            # Increment clicks in the database using F() expressions
            BlogPost.objects.filter(id=blog_id).update(click_count=F("click_count") + click_count)

            # Store updated blog IDs to log
            updated_blogs.append(blog_id)

            # Clear the cache after syncing
            cache.delete(cache_key)

    return f"Synced {len(updated_blogs)} blog clicks to DB."


@shared_task
def sync_blog_conversions_to_db():
    """
    Periodically syncs blog conversion counts from Redis to the database in bulk.
    """
    updated_blogs = []
    cache_keys = cache.keys("blog_conversions_*")

    for cache_key in cache_keys:
        blog_id = cache_key.split("_")[-1]  # Extract blog ID from cache key
        conversion_count = cache.get(cache_key)

        if conversion_count:
            BlogPost.objects.filter(id=blog_id).update(conversion_count=F("conversion_count") + conversion_count)
            updated_blogs.append(blog_id)
            cache.delete(cache_key)

    return f"Synced {len(updated_blogs)} blog conversions to DB."


@shared_task
def cleanup_soft_deleted_blogs():
    """
    Deletes soft-deleted blogs that have been expired for more than 30 days.
    """
    threshold_date = now() - timedelta(days=30)
    expired_blogs = BlogPost.objects.filter(is_deleted=True, deleted_at__lte=threshold_date)
    deleted_count = expired_blogs.count()

    for blog in expired_blogs:
        cache.delete(f"blog_{blog.id}")  # Clear related cache
        cache.delete(f"blog_clicks_{blog.id}")
        cache.delete(f"blog_conversions_{blog.id}")

    expired_blogs.delete()  # Permanently delete from DB

    return f"Deleted {deleted_count} expired soft-deleted blogs."

@shared_task
def update_trending_blogs():
    """Fetches and caches trending and most engaging blogs."""
    last_week = now() - timedelta(days=7)

    trending_blogs = BlogPost.objects.filter(updated_at__gte=last_week).order_by("-click_count")[:5]
    engaging_blogs = BlogPost.objects.filter(updated_at__gte=last_week).order_by("-conversion_count")[:5]

    engagement_stats = {
        "trending_blogs": [{"title": b.title, "slug": b.slug, "clicks": b.click_count} for b in trending_blogs],
        "most_engaging_blogs": [{"title": b.title, "slug": b.slug, "conversions": b.conversion_count} for b in engaging_blogs],
    }

    cache.set("engagement_stats", engagement_stats, timeout=3600)  # Cache for 1 hour
    return "Updated trending blog cache."
